{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb3003-e081-4358-9994-fca3d124354f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import time\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3416a3-4e35-4ce8-814b-8576dcc3420d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(backend=\"huggingface\", region=region)\n",
    "\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e68186-500e-4b17-a8d1-1ebca9571239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g4dn.12xlarge\" # instance type to use for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33ba50-60c9-47e5-bb64-6ecff42742ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "hf_model_id = \"tiiuae/falcon-40b\" # model id from huggingface.co/models\n",
    "number_of_gpu = 4 # number of gpus to use for inference and tensor parallelism\n",
    "health_check_timeout = 600 # Increase the timeout for the health check to 5 minutes for downloading the model\n",
    "falcon_model_name = name_from_base(hf_model_id.split(\"/\")[-1].replace(\".\", \"-\"))\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    name=falcon_model_name,\n",
    "    image_uri=image_uri,\n",
    "    env={\n",
    "        'HF_MODEL_ID': hf_model_id,\n",
    "        # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "        'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "        'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "        #'HF_MODEL_REVISION': 'eb410fb6ffa9028e97adb801f0d6ec46d02f8b07',  \n",
    "        #'HF_MODEL_QUANTIZE': 'bitsandbytes-nf4',\n",
    "        'HF_MODEL_QUANTIZE': 'bitsandbytes-nf4',\n",
    "        'HUGGINGFACE_HUB_CACHE': \"/tmp/huggingface\",  \n",
    "        'SAGEMAKER_CONTAINER_LOG_LEVEL': \"20\"\n",
    "    }  \n",
    ")\n",
    "falcon_endpoint_name = falcon_model_name\n",
    "falcon_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebcc5e-7403-4dc5-9409-dfbef944d4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "falcon_endpoint_name = 'hf-llm-falcon-40b-instruct-bf16-2024-03-11-04-39-29-535'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69901f71-32d6-48d3-8c52-b55377655616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  endpoint_name=falcon_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5e1a2-9ca3-45c2-8023-3ba35ceea966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33ae5b-1e72-4964-83c4-bac0d9503f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a49049-bd5d-4021-8ff5-8ddd0f896480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait for the endpoint to be deployed successfully\n",
    "def wait_for_endpoint(endpoint_name=None):\n",
    "    describe_endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "    while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "        describe_endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "        time.sleep(15)\n",
    "\n",
    "    print(f\"endpoint {endpoint_name} is in service now.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ad461-01d7-4a10-941c-e34dfbadf7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ep_name in [falcon_endpoint_name]:\n",
    "    wait_for_endpoint(ep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43dce0-881d-48dc-9414-bcbb60d18da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def query_endpoint(payload):\n",
    "    # Create a low-level client representing Amazon SageMaker Runtime\n",
    "    sagemaker_runtime = boto3.client(\n",
    "        \"sagemaker-runtime\", region_name='us-east-1')\n",
    "\n",
    "    # Gets inference from the model hosted at the specified endpoint:\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=falcon_endpoint_name, \n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "    # Decodes and prints the response body:\n",
    "    print(response['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d79c0-d151-44b9-971c-fc718692f065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"\"\"Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "    Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "    that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "    started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "    programmatically through the SageMaker Python SDK.\n",
    "    Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "    ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "    benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "    exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "    state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "    Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "    needs for applications such as translation, question answering, and summarizing information.\n",
    "    Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "    US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "    Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "    with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "    please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart â€“\n",
    "    Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "    state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above:\"\"\",\n",
    "    \"parameters\": {\"max_new_tokens\": 200},\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603149d-ac90-4374-89b7-b05f043b9dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me about Amazon SageMaker.\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2bdd34-af64-4209-b447-243fbd8c3492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3926241-35aa-46cc-8a2d-5c3189ec2a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f1dfd-3cb1-4e3d-be48-c13296c8dba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebba86-724f-410c-ba72-a32218d52a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"doc.txt\") as f:\n",
    "    text_to_summarize = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce7cc4-6deb-475e-9036-690e6ed37126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain import SagemakerEndpoint, PromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size = 500,\n",
    "                    chunk_overlap  = 20,\n",
    "                    separators = [\" \"],\n",
    "                    length_function = len\n",
    "                )\n",
    "input_documents = text_splitter.create_documents([text_to_summarize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f32426-c74c-44a2-b1be-4ca50b2bc0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandlerTextSummarization(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> json:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        generated_text = response_json[0]['generated_text']\n",
    "        return generated_text.split(\"summary:\")[-1]\n",
    "    \n",
    "content_handler = ContentHandlerTextSummarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4511b-8c13-47e4-8db1-a87769ef18da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_prompt = \"\"\"Write a concise summary of this text in a few complete sentences:\n",
    "\n",
    "{text}\n",
    "\n",
    "Concise summary:\"\"\"\n",
    "\n",
    "map_prompt_template = PromptTemplate(\n",
    "                        template=map_prompt, \n",
    "                        input_variables=[\"text\"]\n",
    "                      )\n",
    "\n",
    "\n",
    "combine_prompt = \"\"\"Combine all these following summaries and generate a final summary of them in a few complete sentences:\n",
    "\n",
    "{text}\n",
    "\n",
    "Final summary:\"\"\"\n",
    "\n",
    "combine_prompt_template = PromptTemplate(\n",
    "                            template=combine_prompt, \n",
    "                            input_variables=[\"text\"]\n",
    "                          ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657acc4d-7eca-4673-8072-5673f4b1ea6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_model = SagemakerEndpoint(\n",
    "                    endpoint_name = falcon_endpoint_name,\n",
    "                    region_name= \"us-east-1\",\n",
    "                    model_kwargs= {},\n",
    "                    content_handler=content_handler\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e66015-16c9-4a44-b5ad-bc9ca4c9f5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(llm=summary_model,\n",
    "                                     chain_type=\"map_reduce\", \n",
    "                                     map_prompt=map_prompt_template,\n",
    "                                     combine_prompt=combine_prompt_template,\n",
    "                                     verbose=True\n",
    "                                    ) \n",
    "summary = summary_chain({\"input_documents\": input_documents, 'token_max': 700}, return_only_outputs=True)\n",
    "print(summary[\"output_text\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c4507-d329-43fc-8b7d-209d9179cefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "texts = text_splitter.split_text(text_to_summarize) \n",
    "print('texts[0]: ', texts[0])\n",
    "        \n",
    "docs = [\n",
    "            Document(\n",
    "                page_content=t\n",
    "            ) for t in texts[:3]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ef990-0f41-4b43-b00d-b7f20061bbb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "        {text}\n",
    "        \n",
    "        CONCISE SUMMARY \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47088499-17bf-4070-bdd1-6c120fe9de70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = load_summarize_chain(summary_model, chain_type=\"stuff\", prompt=PROMPT)\n",
    "summary = chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affb55a-5814-4b1f-a6f8-92c17e7d716e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b01a2e-cc53-47cf-918a-bd6fddc03ffd",
   "metadata": {},
   "source": [
    "### Created helper function, but conceptually it is the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8d7af-b8da-429f-9656-34b742bfcf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt_template = \"\"\"\n",
    "Please provide a summary of the following text.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "chuck_prompt_template = \"\"\"\n",
    "Please provide a summary of the following text.\n",
    "Please answer in one sentence.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=chuck_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "Write a concise summary of the following text.\n",
    "Return your response in bullet points which covers the key points of the text.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be5083-35e7-492c-a4cc-b576a75e1822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "            summary_model,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=chunk_prompt,\n",
    "            refine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b26b6-ec63-4566-b4d3-bc7c70ddb4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summary_chain_init(chain_type, llm):\n",
    "    \n",
    "    if chain_type == \"STUFF\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    elif chain_type == \"MAP_REDUCE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=chunk_prompt,\n",
    "            combine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif chain_type == \"REFINE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=chunk_prompt,\n",
    "            refine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99629ac7-eedf-4105-881c-48d112eb49db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def long_call_analysis(llm, transcript, params, template=\"\", chain_type=\"MAP_REDUCE\", max_tokens=50):\n",
    "\n",
    "    \n",
    "    llm.model_kwargs = params\n",
    "    num_tokens = llm.get_num_tokens(transcript) #raise warnning\n",
    "\n",
    "    if num_tokens > max_tokens:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\\n\"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        docs = text_splitter.create_documents([transcript])\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n",
    "\n",
    "        \n",
    "        summary_chain = summary_chain_init(\n",
    "            chain_type=chain_type, \n",
    "            llm=llm\n",
    "        )\n",
    "        response = summary_chain(\n",
    "            {\"input_documents\": docs}\n",
    "        )\n",
    "        \n",
    "        print (\"Intermediate_steps: \\n\")\n",
    "        for idx, step in enumerate(response[\"intermediate_steps\"]):\n",
    "            print (colored(f'step {idx}: \\n', \"green\"))\n",
    "            print (colored(f'{step}\\n', \"green\"))\n",
    "        \n",
    "        return response[\"output_text\"]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        prompt = PromptTemplate(template=stuff_prompt_template, input_variables=[\"text\"])\n",
    "        analysis_prompt = prompt.format(text=transcript)\n",
    "        print (colored(analysis_prompt, 'green'))\n",
    "        \n",
    "        response = llm(analysis_prompt)\n",
    "        \n",
    "        return response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf54e4-53c1-46b4-95e0-de686e8e28e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"FALCON-40B\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"max_length\": 2048,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.2,\n",
    "        \"return_full_text\": False,\n",
    "        \"include_prompt_in_result\": False\n",
    "    },\n",
    "    \"LLAMA2-7B\": {\n",
    "        'max_new_tokens': 128,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.1,\n",
    "        'return_full_text': False\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117b2a7-041f-4498-bd32-613ceb7fd808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"FALCON-40B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe3f9d-a2e9-402d-9d62-171342060ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "Analyze the retail support call transcript below. Provide a detail summary of the conversation in complete sentence:\n",
    "\n",
    "context: {transcript}\n",
    "\n",
    "summary:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622a40a-e50c-4c66-83e9-16ba47d81f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5edb225-273f-425c-9a2e-224a4620321b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = long_call_analysis(\n",
    "    llm=summary_model,\n",
    "    transcript=text_to_summarize,\n",
    "    params=PARAMS[MODEL_NAME],\n",
    "    template=summary_template,\n",
    "    chain_type=\"MAP_REDUCE\" # REFINE, MAP_REDUCE\n",
    ")\n",
    "\n",
    "print (\"Results: \\n\")\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b24449-bbd1-4a57-9150-b59bf6c51dc2",
   "metadata": {},
   "source": [
    "## Using Bedrock Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0b348-c63c-4292-a5e0-71b2458c0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "\n",
    "# Create a BedrockRuntime client\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "\n",
    "payload = {\n",
    "    \"modelId\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"contentType\": \"application/json\",\n",
    "    \"accept\": \"application/json\",\n",
    "    \"body\": {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": text_to_summarize\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert the payload to bytes\n",
    "body_bytes = json.dumps(payload['body']).encode('utf-8')\n",
    "\n",
    "# Invoke the model\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body_bytes,\n",
    "    contentType=payload['contentType'],\n",
    "    accept=payload['accept'],\n",
    "    modelId=payload['modelId']\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "response_body = response['body'].read().decode('utf-8')\n",
    "print(response_body)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
